{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS IS USED TO SHOW THE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"procgen:procgen-coinrun-v0\",\n",
    "    num_levels=0,\n",
    "    rand_seed=42,\n",
    "    #start_level=3,\n",
    "    render_mode='rgb_array',  # Allows rendering with RGB output\n",
    "    distribution_mode='easy',\n",
    "    use_backgrounds=False\n",
    ")\n",
    "\n",
    "# Get the initial observation\n",
    "obs = env.reset()\n",
    "\n",
    "# Number of iterations you want to test\n",
    "max_iteration = 1000\n",
    "\n",
    "# Enable interactive plotting\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "for i in range(max_iteration):\n",
    "    # Ensure the observation is in the right shape for the agent\n",
    "    action, logits, value = ppo_agent.select_action(tf.expand_dims(obs, axis=0), training=False)\n",
    "    action = np.squeeze(action, axis=0)\n",
    "\n",
    "    # Take the action in the environment\n",
    "    next_obs, rew, done,  info = env.step(action)\n",
    "    \n",
    "    # Display the environment's rendered image (info['rgb'] for Procgen)\n",
    "    plt.clf()  # Clear the current figure\n",
    "    plt.imshow(info[\"rgb\"])  # Render the RGB frame from the environment\n",
    "    plt.pause(1 / 144)  # Adjust the speed of the loop if necessary\n",
    "    \n",
    "\n",
    "# Close the plotting and the environment\n",
    "plt.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE A FUNCTION TO PLOT REWARDS AND STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences with NaN\n",
    "max_length = max(len(r) for r in episode_rewards_small)\n",
    "padded_rewards = np.array([np.pad(r, (0, max_length - len(r)), constant_values=np.nan) for r in episode_rewards_small])\n",
    "\n",
    "# Compute statistics\n",
    "mean_rewards = np.nanmean(padded_rewards, axis=0)\n",
    "std_rewards = np.nanstd(padded_rewards, axis=0)\n",
    "\n",
    "# Plot\n",
    "episodes = np.arange(len(mean_rewards))\n",
    "plt.plot(episodes, mean_rewards, label='Mean Reward')\n",
    "plt.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2, label='Std Dev')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training Performance with Inhomogeneous Lengths')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sequences(sequences):\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded = np.full((len(sequences), max_length), np.nan, dtype=float)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    \n",
    "    return padded\n",
    "\n",
    "# Convert episode_timesteps to float if it contains integers\n",
    "episode_timesteps = [np.array(t, dtype=float) for t in episode_timesteps_small]\n",
    "\n",
    "# Pad the sequences\n",
    "padded_timesteps = pad_sequences(episode_timesteps)\n",
    "\n",
    "# Compute statistics while ignoring NaN\n",
    "mean_timesteps = np.nanmean(padded_timesteps, axis=0)\n",
    "std_timesteps = np.nanstd(padded_timesteps, axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(mean_timesteps)), mean_timesteps, label='Mean Timesteps', color='blue')\n",
    "plt.fill_between(range(len(mean_timesteps)), \n",
    "                 mean_timesteps - std_timesteps, \n",
    "                 mean_timesteps + std_timesteps, \n",
    "                 alpha=0.2, label='Std Dev', color='blue')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Timesteps')\n",
    "plt.title('Episode Length Over Time')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size\n",
    "window_size = 10\n",
    "\n",
    "# Compute sliding window sums manually\n",
    "windowed_rewards = []\n",
    "for rewards in episode_rewards:\n",
    "    env_windowed = [sum(rewards[i:i+window_size]) for i in range(len(rewards) - window_size + 1)]\n",
    "    windowed_rewards.append(env_windowed)\n",
    "\n",
    "# Plot the sliding window cumulative rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, rewards in enumerate(windowed_rewards):\n",
    "    plt.plot(rewards, label=f\"Env {i+1}\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Episodes (adjusted for window)')\n",
    "plt.ylabel('Cumulative Reward (Sliding Window)')\n",
    "plt.title(f'Cumulative Reward per Environment (Window Size = {window_size})')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all rewards into a single list (ignoring episode boundaries)\n",
    "all_rewards_flattened = [reward for env_rewards in episode_rewards for reward in env_rewards]\n",
    "\n",
    "# Compute cumulative mean\n",
    "cumulative_rewards = np.cumsum(all_rewards_flattened) / np.arange(1, len(all_rewards_flattened) + 1)\n",
    "\n",
    "# Plot\n",
    "plt.plot(cumulative_rewards, label=\"Cumulative Mean Reward\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Mean Reward During Training\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten rewards and compute moving average\n",
    "flat_rewards = [reward for env_rewards in episode_rewards for reward in env_rewards]\n",
    "window_size = 10  # Smoothing window\n",
    "moving_avg = np.convolve(flat_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot\n",
    "plt.plot(moving_avg, label=\"Moving Average (Window=10)\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Moving Average During Training\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume `episode_rewards` is a list of lists where:\n",
    "# - Outer list: Each environment\n",
    "# - Inner list: Rewards per episode for that environment\n",
    "\n",
    "num_envs = len(episode_rewards_small)  # Total number of environments\n",
    "\n",
    "# Compute cumulative rewards per environment\n",
    "cumulative_rewards_per_env = [np.cumsum(rewards) for rewards in episode_rewards_small]\n",
    "\n",
    "# Find max episode length across all environments (to ensure fair plotting)\n",
    "max_episodes = max(len(rewards) for rewards in episode_rewards_small)\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Plot cumulative rewards for each environment\n",
    "for i in range(num_envs):\n",
    "    plt.plot(range(len(cumulative_rewards_per_env[i])), cumulative_rewards_per_env[i], \n",
    "             label=f\"Env {i+1}\", alpha=0.6)\n",
    "\n",
    "# Compute and plot the average cumulative reward across all environments\n",
    "avg_cumulative_rewards = np.zeros(max_episodes)\n",
    "\n",
    "# Normalize environments with different episode lengths\n",
    "for env_rewards in cumulative_rewards_per_env:\n",
    "    padded_rewards = np.pad(env_rewards, (0, max_episodes - len(env_rewards)), mode='edge')  # Extend last value\n",
    "    avg_cumulative_rewards += padded_rewards\n",
    "\n",
    "avg_cumulative_rewards /= num_envs  # Get average across environments\n",
    "\n",
    "# Plot the averaged cumulative reward trend\n",
    "plt.plot(range(max_episodes), avg_cumulative_rewards, \n",
    "         label=\"Average Cumulative Reward\", color='black', linewidth=2, linestyle=\"dashed\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward Across Multiple Environments\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABLATION STUDY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coinrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/250000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.3803576, shape=(), dtype=float32) tf.Tensor(0.1077861, shape=(), dtype=float32) tf.Tensor(0.54959357, shape=(), dtype=float32) tf.Tensor(2.2252672, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17356285, shape=(), dtype=float32) tf.Tensor(0.05766269, shape=(), dtype=float32) tf.Tensor(0.23625734, shape=(), dtype=float32) tf.Tensor(2.2284946, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 16192/250000 [05:13<1:15:30, 51.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.14566384, shape=(), dtype=float32) tf.Tensor(0.03285703, shape=(), dtype=float32) tf.Tensor(0.23009987, shape=(), dtype=float32) tf.Tensor(2.2431223, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 16192/250000 [05:30<1:15:30, 51.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.19627212, shape=(), dtype=float32) tf.Tensor(0.08718929, shape=(), dtype=float32) tf.Tensor(0.22256605, shape=(), dtype=float32) tf.Tensor(2.2001953, shape=(), dtype=float32)\n",
      "tf.Tensor(0.08112828, shape=(), dtype=float32) tf.Tensor(0.010688776, shape=(), dtype=float32) tf.Tensor(0.14537112, shape=(), dtype=float32) tf.Tensor(2.2460592, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 32384/250000 [10:28<1:10:24, 51.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.04992461, shape=(), dtype=float32) tf.Tensor(0.00697182, shape=(), dtype=float32) tf.Tensor(0.090287104, shape=(), dtype=float32) tf.Tensor(2.1907656, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 32384/250000 [10:40<1:10:24, 51.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.18822557, shape=(), dtype=float32) tf.Tensor(0.090319075, shape=(), dtype=float32) tf.Tensor(0.20021865, shape=(), dtype=float32) tf.Tensor(2.2028344, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12838812, shape=(), dtype=float32) tf.Tensor(0.05718423, shape=(), dtype=float32) tf.Tensor(0.14690363, shape=(), dtype=float32) tf.Tensor(2.2479305, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 48576/250000 [15:42<1:05:09, 51.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.10222208, shape=(), dtype=float32) tf.Tensor(0.03802047, shape=(), dtype=float32) tf.Tensor(0.1329023, shape=(), dtype=float32) tf.Tensor(2.2495375, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 48576/250000 [16:00<1:05:09, 51.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.20216988, shape=(), dtype=float32) tf.Tensor(0.120112024, shape=(), dtype=float32) tf.Tensor(0.16858931, shape=(), dtype=float32) tf.Tensor(2.2367988, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15653566, shape=(), dtype=float32) tf.Tensor(0.03688671, shape=(), dtype=float32) tf.Tensor(0.2438304, shape=(), dtype=float32) tf.Tensor(2.266261, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 64768/250000 [20:59<1:00:07, 51.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.123783015, shape=(), dtype=float32) tf.Tensor(0.04841705, shape=(), dtype=float32) tf.Tensor(0.15529925, shape=(), dtype=float32) tf.Tensor(2.2836525, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 64768/250000 [21:11<1:00:07, 51.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.24731721, shape=(), dtype=float32) tf.Tensor(0.11849773, shape=(), dtype=float32) tf.Tensor(0.26217297, shape=(), dtype=float32) tf.Tensor(2.2669978, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16105954, shape=(), dtype=float32) tf.Tensor(0.058638334, shape=(), dtype=float32) tf.Tensor(0.20940737, shape=(), dtype=float32) tf.Tensor(2.2824788, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 80960/250000 [26:23<55:22, 50.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.121806055, shape=(), dtype=float32) tf.Tensor(0.0409893, shape=(), dtype=float32) tf.Tensor(0.16621813, shape=(), dtype=float32) tf.Tensor(2.292306, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 80960/250000 [26:41<55:22, 50.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.28165394, shape=(), dtype=float32) tf.Tensor(0.08769362, shape=(), dtype=float32) tf.Tensor(0.3924212, shape=(), dtype=float32) tf.Tensor(2.2502499, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1760558, shape=(), dtype=float32) tf.Tensor(0.05372891, shape=(), dtype=float32) tf.Tensor(0.24922895, shape=(), dtype=float32) tf.Tensor(2.287585, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 97152/250000 [31:41<50:05, 50.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.121780425, shape=(), dtype=float32) tf.Tensor(0.019077789, shape=(), dtype=float32) tf.Tensor(0.20997629, shape=(), dtype=float32) tf.Tensor(2.2855077, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 97152/250000 [31:51<50:05, 50.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.22939044, shape=(), dtype=float32) tf.Tensor(0.115367055, shape=(), dtype=float32) tf.Tensor(0.232481, shape=(), dtype=float32) tf.Tensor(2.2171078, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17951477, shape=(), dtype=float32) tf.Tensor(0.060148485, shape=(), dtype=float32) tf.Tensor(0.24325597, shape=(), dtype=float32) tf.Tensor(2.2616935, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 113344/250000 [36:55<44:35, 51.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.09196188, shape=(), dtype=float32) tf.Tensor(0.015021453, shape=(), dtype=float32) tf.Tensor(0.15841427, shape=(), dtype=float32) tf.Tensor(2.2667034, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 113344/250000 [37:12<44:35, 51.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.3411216, shape=(), dtype=float32) tf.Tensor(0.09700309, shape=(), dtype=float32) tf.Tensor(0.49273768, shape=(), dtype=float32) tf.Tensor(2.2503223, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17341048, shape=(), dtype=float32) tf.Tensor(0.031313688, shape=(), dtype=float32) tf.Tensor(0.28867388, shape=(), dtype=float32) tf.Tensor(2.2401586, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 129536/250000 [42:19<39:33, 50.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.1342093, shape=(), dtype=float32) tf.Tensor(-0.016582828, shape=(), dtype=float32) tf.Tensor(0.30610442, shape=(), dtype=float32) tf.Tensor(2.260074, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 129536/250000 [42:32<39:33, 50.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.31619933, shape=(), dtype=float32) tf.Tensor(0.09019977, shape=(), dtype=float32) tf.Tensor(0.4564766, shape=(), dtype=float32) tf.Tensor(2.238712, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14103168, shape=(), dtype=float32) tf.Tensor(0.013478826, shape=(), dtype=float32) tf.Tensor(0.2596233, shape=(), dtype=float32) tf.Tensor(2.2587948, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  58%|█████▊    | 145728/250000 [47:48<34:34, 50.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.14940317, shape=(), dtype=float32) tf.Tensor(0.048064757, shape=(), dtype=float32) tf.Tensor(0.20716433, shape=(), dtype=float32) tf.Tensor(2.2437553, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  58%|█████▊    | 145728/250000 [48:02<34:34, 50.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.32649916, shape=(), dtype=float32) tf.Tensor(0.1290068, shape=(), dtype=float32) tf.Tensor(0.3994402, shape=(), dtype=float32) tf.Tensor(2.2277184, shape=(), dtype=float32)\n",
      "tf.Tensor(0.19258025, shape=(), dtype=float32) tf.Tensor(0.03496183, shape=(), dtype=float32) tf.Tensor(0.31976143, shape=(), dtype=float32) tf.Tensor(2.262294, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▍   | 161920/250000 [53:19<29:27, 49.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.16405383, shape=(), dtype=float32) tf.Tensor(-0.001392436, shape=(), dtype=float32) tf.Tensor(0.33543015, shape=(), dtype=float32) tf.Tensor(2.2688103, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▍   | 161920/250000 [53:33<29:27, 49.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.37653816, shape=(), dtype=float32) tf.Tensor(0.12325408, shape=(), dtype=float32) tf.Tensor(0.51099825, shape=(), dtype=float32) tf.Tensor(2.2150578, shape=(), dtype=float32)\n",
      "tf.Tensor(0.28411338, shape=(), dtype=float32) tf.Tensor(0.05600507, shape=(), dtype=float32) tf.Tensor(0.46068612, shape=(), dtype=float32) tf.Tensor(2.2347677, shape=(), dtype=float32)\n",
      "tf.Tensor(0.18141885, shape=(), dtype=float32) tf.Tensor(0.028625365, shape=(), dtype=float32) tf.Tensor(0.31005138, shape=(), dtype=float32) tf.Tensor(2.2322125, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|███████   | 178112/250000 [59:13<24:15, 49.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.27568576, shape=(), dtype=float32) tf.Tensor(0.09616268, shape=(), dtype=float32) tf.Tensor(0.36352724, shape=(), dtype=float32) tf.Tensor(2.2405496, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15495631, shape=(), dtype=float32) tf.Tensor(0.03667896, shape=(), dtype=float32) tf.Tensor(0.24104743, shape=(), dtype=float32) tf.Tensor(2.246367, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  78%|███████▊  | 194304/250000 [1:04:35<19:01, 48.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.20381896, shape=(), dtype=float32) tf.Tensor(0.056141965, shape=(), dtype=float32) tf.Tensor(0.2998392, shape=(), dtype=float32) tf.Tensor(2.2426102, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  78%|███████▊  | 194304/250000 [1:04:53<19:01, 48.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.40274164, shape=(), dtype=float32) tf.Tensor(0.08536393, shape=(), dtype=float32) tf.Tensor(0.6391721, shape=(), dtype=float32) tf.Tensor(2.2083268, shape=(), dtype=float32)\n",
      "tf.Tensor(0.29549685, shape=(), dtype=float32) tf.Tensor(0.03602276, shape=(), dtype=float32) tf.Tensor(0.52343255, shape=(), dtype=float32) tf.Tensor(2.2421718, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▍ | 210496/250000 [1:11:06<14:14, 46.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.24169193, shape=(), dtype=float32) tf.Tensor(0.024477493, shape=(), dtype=float32) tf.Tensor(0.43890938, shape=(), dtype=float32) tf.Tensor(2.2402594, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▍ | 210496/250000 [1:11:23<14:14, 46.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.31556097, shape=(), dtype=float32) tf.Tensor(0.08099713, shape=(), dtype=float32) tf.Tensor(0.47358236, shape=(), dtype=float32) tf.Tensor(2.2273273, shape=(), dtype=float32)\n",
      "tf.Tensor(0.26754752, shape=(), dtype=float32) tf.Tensor(0.04961814, shape=(), dtype=float32) tf.Tensor(0.4403805, shape=(), dtype=float32) tf.Tensor(2.26088, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████ | 226688/250000 [1:18:02<08:52, 43.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.18966705, shape=(), dtype=float32) tf.Tensor(0.02353039, shape=(), dtype=float32) tf.Tensor(0.33678865, shape=(), dtype=float32) tf.Tensor(2.2576823, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████ | 226688/250000 [1:18:14<08:52, 43.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4549195, shape=(), dtype=float32) tf.Tensor(0.096778825, shape=(), dtype=float32) tf.Tensor(0.720748, shape=(), dtype=float32) tf.Tensor(2.2333426, shape=(), dtype=float32)\n",
      "tf.Tensor(0.21981335, shape=(), dtype=float32) tf.Tensor(0.028342655, shape=(), dtype=float32) tf.Tensor(0.38739997, shape=(), dtype=float32) tf.Tensor(2.2292843, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  97%|█████████▋| 242880/250000 [1:25:19<02:51, 41.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.22419137, shape=(), dtype=float32) tf.Tensor(-0.0136071, shape=(), dtype=float32) tf.Tensor(0.48007393, shape=(), dtype=float32) tf.Tensor(2.2384968, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  97%|█████████▋| 242880/250000 [1:27:53<02:34, 46.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to 250k_coinrun_andAdream_v2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Agent: 100%|██████████| 25000/25000 [06:21<00:00, 65.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Agent Test Results: Avg Reward: 5.27, Std Reward: 4.99, Avg Length: 149.05, in 165 episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import make_env, test_agent, load_models\n",
    "from ppo import PPOAgent \n",
    "\n",
    "seed = 70\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_envs = 8\n",
    "envs = gym.vector.AsyncVectorEnv([make_env(seed=seed + i,) for i in range(num_envs)])\n",
    "observation_space = envs.single_observation_space._shape\n",
    "action_space = envs.single_action_space.n\n",
    "\n",
    "impala_filters = [16, 32, 32]\n",
    "dense_units = 512\n",
    "\n",
    "buffer_size = 2024  # Default buffer size\n",
    "learning_rate = 1e-4\n",
    "minibatch = 512 # Default minibatch\n",
    "epochs = 3\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95 #0.9\n",
    "epsilon = 0.1 #0.1  \n",
    "entropy_coeff = 0.001#0.01\n",
    "\n",
    "# PPO Agent with epsilon 0.1\n",
    "ppo_agent_epsilon_0_1 = PPOAgent(\n",
    "    envs=envs,\n",
    "    num_actions=action_space,\n",
    "    obs_shape=observation_space,\n",
    "    impala_filters=impala_filters,\n",
    "    dense_units=dense_units,\n",
    "    num_envs=num_envs,\n",
    "    buffer_size=buffer_size,\n",
    "    learning_rate=learning_rate,\n",
    "    minibatch=minibatch,\n",
    "    epochs=epochs,\n",
    "    entropy_coeff=entropy_coeff,\n",
    "    model_name=\"250k_coinrun_andAdream_v2\",\n",
    ")\n",
    "\n",
    "max_timesteps = 250000  \n",
    "episode_rewards_epsilon_0_1, episode_timesteps_epsilon_0_1 = ppo_agent_epsilon_0_1.train(max_timesteps=max_timesteps)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(ppo_agent_epsilon_0_1.model_name+'.txt', 'w') as f:\n",
    "    f.write(\"Episode Rewards:\\n\")\n",
    "    for rewards in episode_rewards_epsilon_0_1:\n",
    "        f.write(f\"{rewards}\\n\")\n",
    "    \n",
    "    f.write(\"\\nEpisode Timesteps:\\n\")\n",
    "    for timesteps in episode_timesteps_epsilon_0_1:\n",
    "        f.write(f\"{timesteps}\\n\")\n",
    "\n",
    "print(f\"Results have been written to {ppo_agent_epsilon_0_1.model_name+'.txt'}\")\n",
    "\n",
    "seeds = [12,123,62,91] \n",
    "test_envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(seed=seed,num_levels=0,start_level=seed) for seed in (seeds)]\n",
    ")\n",
    "\n",
    "test_results_ppo = test_agent(agent=ppo_agent_epsilon_0_1, envs=test_envs, total_timesteps=25000, agent_type=\"ppo\")\n",
    "#test_results_random = test_agent(agent=None, envs=test_envs, total_timesteps=10000, agent_type=\"random\")\n",
    "\n",
    "#del ppo_agent_epsilon_0_1  # Delete the PPO agent object\n",
    "#del envs  # Delete the environment object\n",
    "#del test_envs\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|█████████▉| 199680/200000 [2:13:42<00:12, 24.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to 250k_coinrun_andAdream_but_small_lr1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Agent: 100%|██████████| 20000/20000 [04:04<00:00, 81.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Agent Test Results: Avg Reward: 5.03, Std Reward: 5.00, Avg Length: 108.20, in 183 episodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import make_env, test_agent, load_models\n",
    "from ppo import PPOAgent \n",
    "\n",
    "seed = 70\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_envs = 8\n",
    "envs = gym.vector.AsyncVectorEnv([make_env(seed=seed + i,) for i in range(num_envs)])\n",
    "observation_space = envs.single_observation_space._shape\n",
    "action_space = envs.single_action_space.n\n",
    "\n",
    "impala_filters = [16, 32, 32]\n",
    "dense_units = 512\n",
    "\n",
    "buffer_size = 128#256  # Default buffer size\n",
    "learning_rate = 1e-5\n",
    "minibatch = 128#128*num_envs//8 # Default minibatch\n",
    "epochs = 6\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95 #0.9\n",
    "epsilon = 0.1 #0.1  \n",
    "entropy_coeff = 0.01#0.01\n",
    "\n",
    "# PPO Agent with epsilon 0.1\n",
    "ppo_agent_small = PPOAgent(\n",
    "    envs=envs,\n",
    "    num_actions=action_space,\n",
    "    obs_shape=observation_space,\n",
    "    impala_filters=impala_filters,\n",
    "    dense_units=dense_units,\n",
    "    num_envs=num_envs,\n",
    "    buffer_size=buffer_size,\n",
    "    learning_rate=learning_rate,\n",
    "    minibatch=minibatch,\n",
    "    epochs=epochs,\n",
    "    entropy_coeff=entropy_coeff,\n",
    "    model_name=\"250k_coinrun_andAdream_but_small_lr1\",\n",
    ")\n",
    "\n",
    "max_timesteps = 200000  \n",
    "episode_rewards_small, episode_timesteps_small = ppo_agent_small.train(max_timesteps=max_timesteps)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(ppo_agent_small.model_name+'.txt', 'w') as f:\n",
    "    f.write(\"Episode Rewards:\\n\")\n",
    "    for rewards in episode_rewards_small:\n",
    "        f.write(f\"{rewards}\\n\")\n",
    "    \n",
    "    f.write(\"\\nEpisode Timesteps:\\n\")\n",
    "    for timesteps in episode_timesteps_small:\n",
    "        f.write(f\"{timesteps}\\n\")\n",
    "\n",
    "print(f\"Results have been written to {ppo_agent_small.model_name+'.txt'}\")\n",
    "\n",
    "seeds = [12,123,62,91] \n",
    "test_envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(seed=seed,num_levels=0,start_level=seed) for seed in (seeds)]\n",
    ")\n",
    "\n",
    "test_results_ppo = test_agent(agent=ppo_agent_small, envs=test_envs, total_timesteps=20000, agent_type=\"ppo\")\n",
    "#test_results_random = test_agent(agent=None, envs=test_envs, total_timesteps=10000, agent_type=\"random\")\n",
    "\n",
    "del ppo_agent_small  # Delete the PPO agent object\n",
    "del envs  # Delete the environment object\n",
    "del test_envs\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|█████████▉| 199680/200000 [1:23:41<00:08, 39.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to 250k_coinrun_andAdream_but_small_3epochs.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Agent: 100%|██████████| 20000/20000 [04:32<00:00, 73.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Agent Test Results: Avg Reward: 5.05, Std Reward: 5.00, Avg Length: 166.24, in 111 episodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import make_env, test_agent, load_models\n",
    "from ppo import PPOAgent \n",
    "\n",
    "seed = 70\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_envs = 8\n",
    "envs = gym.vector.AsyncVectorEnv([make_env(seed=seed + i,) for i in range(num_envs)])\n",
    "observation_space = envs.single_observation_space._shape\n",
    "action_space = envs.single_action_space.n\n",
    "\n",
    "impala_filters = [16, 32, 32]\n",
    "dense_units = 512\n",
    "\n",
    "buffer_size = 128#256  # Default buffer size\n",
    "learning_rate = 1e-4\n",
    "minibatch = 128#128*num_envs//8 # Default minibatch\n",
    "epochs = 3\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "epsilon = 0.1  \n",
    "entropy_coeff = 0.01\n",
    "\n",
    "# PPO Agent with epsilon 0.1\n",
    "ppo_agent_small = PPOAgent(\n",
    "    envs=envs,\n",
    "    num_actions=action_space,\n",
    "    obs_shape=observation_space,\n",
    "    impala_filters=impala_filters,\n",
    "    dense_units=dense_units,\n",
    "    num_envs=num_envs,\n",
    "    buffer_size=buffer_size,\n",
    "    learning_rate=learning_rate,\n",
    "    minibatch=minibatch,\n",
    "    epochs=epochs,\n",
    "    entropy_coeff=entropy_coeff,\n",
    "    model_name=\"250k_coinrun_andAdream_but_small_3epochs\",\n",
    ")\n",
    "\n",
    "max_timesteps = 200000  \n",
    "episode_rewards_small, episode_timesteps_small = ppo_agent_small.train(max_timesteps=max_timesteps)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(ppo_agent_small.model_name+'.txt', 'w') as f:\n",
    "    f.write(\"Episode Rewards:\\n\")\n",
    "    for rewards in episode_rewards_small:\n",
    "        f.write(f\"{rewards}\\n\")\n",
    "    \n",
    "    f.write(\"\\nEpisode Timesteps:\\n\")\n",
    "    for timesteps in episode_timesteps_small:\n",
    "        f.write(f\"{timesteps}\\n\")\n",
    "\n",
    "print(f\"Results have been written to {ppo_agent_small.model_name+'.txt'}\")\n",
    "\n",
    "seeds = [12,123,62,91] \n",
    "test_envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(seed=seed,num_levels=0,start_level=seed) for seed in (seeds)]\n",
    ")\n",
    "\n",
    "test_results_ppo = test_agent(agent=ppo_agent_small, envs=test_envs, total_timesteps=20000, agent_type=\"ppo\")\n",
    "#test_results_random = test_agent(agent=None, envs=test_envs, total_timesteps=10000, agent_type=\"random\")\n",
    "del ppo_agent_small  # Delete the PPO agent object\n",
    "del envs  # Delete the environment object\n",
    "del test_envs\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
